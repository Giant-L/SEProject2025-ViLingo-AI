import os
import torch
from datasets import load_dataset, Audio
from transformers import (
    WhisperForConditionalGeneration,
    WhisperProcessor,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
)
from peft import get_peft_model, LoraConfig, TaskType
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# ====================================================================================
# ğŸ’¡ æ ¸å¿ƒä¿®æ”¹ï¼šåŠ¨æ€è·¯å¾„ç®¡ç†
# æ— è®ºä»å“ªé‡Œè¿è¡Œæ­¤è„šæœ¬ï¼Œéƒ½èƒ½ç¡®ä¿è·¯å¾„æ­£ç¡®
# ====================================================================================
# 1. è·å–æ­¤è„šæœ¬æ–‡ä»¶æ‰€åœ¨çš„ç»å¯¹è·¯å¾„
script_path = os.path.abspath(__file__)
# 2. ä»è„šæœ¬è·¯å¾„æ¨æ–­å‡ºé¡¹ç›®æ ¹ç›®å½• (finetune_dataset ç›®å½•çš„ä¸Šçº§)
project_root = os.path.dirname(os.path.dirname(script_path))
print(f"âœ… é¡¹ç›®æ ¹ç›®å½•å·²è‡ªåŠ¨è®¾ç½®ä¸º: {project_root}")

# 3. åŸºäºé¡¹ç›®æ ¹ç›®å½•å®šä¹‰æ‰€æœ‰å…¶ä»–è·¯å¾„
data_file = os.path.join(project_root, "finetune_dataset", "train.jsonl")
cache_dir = os.path.join(project_root, ".cache") # å»ºè®®å°†cacheæ”¾åœ¨æ ¹ç›®å½•
output_dir = os.path.join(project_root, "finetune_dataset", "output")
save_dir = os.path.join(project_root, "finetune_dataset", "finetuned-whisper-lora")
# ====================================================================================


# Step 1: åŠ è½½ Whisper æ¨¡å‹å’Œå¤„ç†å™¨
model_name = "openai/whisper-small"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)

# Step 2: åŠ è½½æ•°æ®é›† (ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„åŠ¨æ€è·¯å¾„)
print(f"â³ æ­£åœ¨ä» '{data_file}' åŠ è½½æ•°æ®é›†...")
dataset = load_dataset("json", data_files=data_file, split="train", cache_dir=cache_dir)
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
print("âœ… æ•°æ®é›†åŠ è½½å®Œæˆ")

# Step 3: æ·»åŠ  LoRA adapter
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.SEQ_2_SEQ_LM
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# Step 4: æ•°æ®é¢„å¤„ç†å‡½æ•°
def prepare_dataset(batch):
    # æ³¨æ„ï¼šè¿™é‡Œçš„ audio è·¯å¾„æ˜¯ç›¸å¯¹ 'data_file' çš„ï¼Œdatasets åº“ä¼šè‡ªåŠ¨å¤„ç†
    # æˆ‘ä»¬ä¸éœ€è¦åœ¨è¿™é‡Œä¿®æ”¹å®ƒ
    audio = batch["audio"]
    batch["input_features"] = processor(audio["array"], sampling_rate=audio["sampling_rate"]).input_features[0]
    batch["labels"] = processor.tokenizer(batch["text"]).input_ids
    return batch

# Step 5: æ˜ å°„é¢„å¤„ç†
processed_dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names, num_proc=1)

# Step 6: æ•°æ®æ•´ç†å™¨ (è¿™éƒ¨åˆ†ä»£ç æ˜¯æ­£ç¡®çš„ï¼Œä¿æŒä¸å˜)
@dataclass
class DataCollatorSpeechSeq2SeqWithPadding:
    processor: Any
    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        input_features = [{"input_features": feature["input_features"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]
        batch = self.processor.feature_extractor.pad(input_features, return_tensors="pt")
        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors="pt")
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)
        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().item():
            labels = labels[:, 1:]
        batch["labels"] = labels
        return batch

# Step 7: å®ä¾‹åŒ–æ•°æ®æ•´ç†å™¨
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)


# Step 8: è‡ªå®šä¹‰ Trainer (ä¿æŒä¸å˜)
class CustomSeq2SeqTrainer(Seq2SeqTrainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        if 'input_ids' in inputs:
            del inputs['input_ids']
        return super().compute_loss(model, inputs, return_outputs)

# Step 9: å®šä¹‰è®­ç»ƒå‚æ•° (ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„åŠ¨æ€è·¯å¾„)
use_fp16 = torch.cuda.is_available()

training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=1e-4,
    num_train_epochs=5,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
    evaluation_strategy="no",
    fp16=use_fp16,
    report_to="none",
    push_to_hub=False,
)

# Step 10: åˆ›å»ºæˆ‘ä»¬è‡ªå®šä¹‰çš„ Trainer å®ä¾‹
trainer = CustomSeq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=processed_dataset,
    data_collator=data_collator,
    tokenizer=processor.feature_extractor,
)

print("\n--- å¼€å§‹è®­ç»ƒ ---")
trainer.train()

# Step 11: ä¿å­˜æ¨¡å‹ (ä½¿ç”¨æˆ‘ä»¬å®šä¹‰çš„åŠ¨æ€è·¯å¾„)
print(f"\nâœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å°†ä¿å­˜åˆ°: {save_dir}")
model.save_pretrained(save_dir)
processor.save_pretrained(save_dir)
print("ğŸ‰ æ¨¡å‹å’Œå¤„ç†å™¨å·²æˆåŠŸä¿å­˜!")